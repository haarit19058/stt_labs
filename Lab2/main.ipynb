{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f8ca1d",
   "metadata": {},
   "source": [
    "## Selecting a repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79e6574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'PDFMathTranslate' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Byaidu/PDFMathTranslate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f79b35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = 'PDFMathTranslate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f8dcc",
   "metadata": {},
   "source": [
    "## Bug-Fixing Commit identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd79b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydriller import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc67dc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 335 potential bug-fixing commits to potential_bug_fix_commits.csv\n"
     ]
    }
   ],
   "source": [
    "# naive approach of simple word matching\n",
    "\n",
    "bug_keywords = [\"fixed\",\"bug\",\"fixes\",\"fix\",\"crash\",\"solves\",\n",
    "                \"resolves\",\"issue\",\"regression\",\"fall back\",\n",
    "                \"assertion\",\"coverity\",\"reproducible\",\"stack-wanted\",\n",
    "                \"steps-wanted\",\"testcase\",\"failure\",\"fail\",\"npe\",\n",
    "                \"except\",\"broken\",\"differential testing\",\"error\",\n",
    "                \"hang\",\"test fix\",\"steps to reproduce\",\"leak\",\n",
    "                \"stack trace\",\"heap overflow\",\"freez\",\"problem\",\n",
    "                \"overflow\",\"avoid\",\"workaround\",\"break\",\"stop\"]\n",
    "\n",
    "\n",
    "def is_bug_commit_naive(commit):\n",
    "    message = commit.msg\n",
    "    msg_lower = message.lower()\n",
    "    return any(kw in msg_lower for kw in bug_keywords)\n",
    "\n",
    "# regex based matching\n",
    "import re\n",
    "\n",
    "# Build one regex from all keywords\n",
    "pattern = re.compile(\n",
    "    r\"\\b(\" + \"|\".join(re.escape(kw) for kw in bug_keywords) + r\")\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# not used in final version\n",
    "def is_bug_commit_regex(commit):\n",
    "    return bool(pattern.search(commit.msg))\n",
    "\n",
    "\n",
    "def is_merge_commit(commit):\n",
    "    return len(commit.parents) > 1\n",
    "\n",
    "\n",
    "commit_data = []\n",
    "\n",
    "for commit in Repository(repo_url).traverse_commits():\n",
    "    diff = ''\n",
    "    for file in commit.modified_files:\n",
    "        diff += file.diff\n",
    "    if is_bug_commit_naive(commit):\n",
    "        commit_data.append({\n",
    "            \"hash\": commit.hash,\n",
    "            \"message\": commit.msg,\n",
    "            \"parents\": commit.parents,\n",
    "            \"is_merge\": commit.merge,\n",
    "            \"diff\": diff,\n",
    "            \"files_modified\": [mod.filename for mod in commit.modified_files]\n",
    "        })\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.DataFrame(commit_data)\n",
    "csv_path = \"potential_bug_fix_commits.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(df)} potential bug-fixing commits to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f0664",
   "metadata": {},
   "source": [
    "## Diff extraction and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd7c287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3406918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(file):\n",
    "    input_text = f\"{file.source_code_before};{file.source_code}\"\n",
    "\n",
    "    # tokenize the and generate \n",
    "    inputs = tokenizer(input_text,return_tensors='pt')\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # decode and generated tokens\n",
    "    prediction = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db467c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 510 potential bug-fixing commits to llm_inference.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pydriller import Repository\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Load model once\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\").to(device)\n",
    "model.eval()\n",
    "\n",
    "MAX_INPUT_TOKENS = 512  \n",
    "MAX_OUTPUT_TOKENS = 512 \n",
    "\n",
    "def safe_infer(diff_text):\n",
    "    \"\"\"Run model inference safely using only file.diff.\"\"\"\n",
    "    if not diff_text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Tokenize & truncate\n",
    "    inputs = tokenizer(\n",
    "        diff_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=MAX_OUTPUT_TOKENS)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def collect_commit_data(repo_url):\n",
    "    iter = 1000000000\n",
    "    commit_data = []\n",
    "    \n",
    "    for commit in Repository(repo_url).traverse_commits():\n",
    "        if iter == 0:\n",
    "            return commit_data\n",
    "        iter-=1\n",
    "        if is_bug_commit_naive(commit):\n",
    "            for file in commit.modified_files:\n",
    "                try:\n",
    "                    inference = safe_infer(file.diff)\n",
    "\n",
    "                    commit_data.append({\n",
    "                        \"hash\": commit.hash,\n",
    "                        \"filename\": file.filename,\n",
    "                        \"diff\": file.diff or \"\",\n",
    "                        \"llm_inference\": inference,\n",
    "                        \"rectified_msg\": \"\"\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped file {file.filename} in commit {commit.hash}: {e}\")\n",
    "                \n",
    "                # Free memory\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return commit_data\n",
    "\n",
    "\n",
    "\n",
    "commit_data = collect_commit_data(repo_url)\n",
    "\n",
    "df = pd.DataFrame(commit_data)\n",
    "csv_path = \"llm_inference.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(df)} potential bug-fixing files to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60962ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (0.31.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from groq) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/haarit/miniconda3/envs/pystt/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3254a28",
   "metadata": {},
   "source": [
    "## Rectifier Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 335 commit-level entries to rectified_msg1.csv\n",
      "Columns: ['hash', 'developer_msg', 'rectified_commit_msg']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from pydriller import Repository\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from groq import Groq  \n",
    "import os\n",
    "\n",
    "import secret.api_key\n",
    "\n",
    "groq_client = Groq(api_key=api_key)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mamiksik/CommitPredictorT5\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mamiksik/CommitPredictorT5\").to(device)\n",
    "model.eval()\n",
    "\n",
    "MAX_INPUT_TOKENS = 512\n",
    "MAX_OUTPUT_TOKENS = 128\n",
    "\n",
    "\n",
    "def infer(diff_text):\n",
    "    \n",
    "    if not diff_text or not diff_text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        diff_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=MAX_INPUT_TOKENS,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=MAX_OUTPUT_TOKENS)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def safe_truncate(text, max_chars=4000):\n",
    "    \n",
    "    if len(text) > max_chars:\n",
    "        return text[:max_chars] + \"\\n...[TRUNCATED]...\"\n",
    "    return text\n",
    "\n",
    "def groq_commit_message(prompt):\n",
    "    \n",
    "    prompt = safe_truncate(prompt, max_chars=4000)  # ~4000 chars ≈ 1500–2000 tokens\n",
    "\n",
    "    chat_completion = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert software engineer skilled at writing precise commit messages.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def collect_commit_data(repo_url, limit=100):\n",
    "    commit_data = []\n",
    "\n",
    "    for commit in Repository(repo_url).traverse_commits():\n",
    "        if limit == 0:\n",
    "            break\n",
    "        limit -= 1\n",
    "\n",
    "        if is_bug_commit_naive(commit):\n",
    "            file_tags = []\n",
    "\n",
    "            for file in commit.modified_files:\n",
    "                try:\n",
    "                    llm_tag = infer(file.diff or \"\")\n",
    "                    file_tags.append(f\"{file.filename}: {llm_tag}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipped file {file.filename} in commit {commit.hash}: {e}\")\n",
    "                \n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            combined_prompt = (\n",
    "                f\"Original commit message:\\n{safe_truncate(commit.msg, 500)}\\n\\n\" +\n",
    "                f\"Changes in files in format <file name>:<change message>\\n\" + \n",
    "                safe_truncate(\"\\n\".join(file_tags), 3000) +\n",
    "                \"\\nTask: Write a concise, accurate commit message summarizing all changes. Respond in a single line only commit message.\"\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                rectified_commit_msg = groq_commit_message(combined_prompt)\n",
    "            except Exception as e:\n",
    "                rectified_commit_msg = f\"[ERROR: {e}]\"\n",
    "\n",
    "            commit_data.append({\n",
    "                \"hash\": commit.hash,\n",
    "                \"developer_msg\": commit.msg,\n",
    "                \"rectified_commit_msg\": rectified_commit_msg\n",
    "            })\n",
    "            # print(rectified_commit_msg)\n",
    "\n",
    "    return commit_data\n",
    "\n",
    "\n",
    "commit_data = collect_commit_data(repo_url, limit=1e8)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(commit_data)\n",
    "csv_path = \"rectified_msg.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nSaved {len(df)} commit-level entries to {csv_path}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "42a6a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file1 = pd.read_csv(\"rectified_msg.csv\")\n",
    "file2 = pd.read_csv('llm_inference.csv')\n",
    "file3 = pd.read_csv('potential_bug_fix_commits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "af74b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 01 colums\n",
      "Index(['hash', 'developer_msg', 'rectified_commit_msg'], dtype='object')\n",
      "file 2 colums\n",
      "Index(['hash', 'filename', 'diff', 'llm_inference', 'rectified_msg'], dtype='object')\n",
      "file3 colums\n",
      "Index(['hash', 'message', 'parents', 'is_merge', 'diff', 'files_modified'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('file 01 colums')\n",
    "print(file1.columns)\n",
    "\n",
    "print(\"file 2 colums\")\n",
    "print(file2.columns)\n",
    "\n",
    "print('file3 colums')\n",
    "print(file3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dd63a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master DataFrame created and saved to master_commits.csv\n",
      "Columns: ['hash', 'developer_msg', 'rectified_commit_msg', 'filename', 'llm_inference', 'diff_x', 'rectified_msg', 'message', 'parents', 'is_merge', 'diff_y', 'files_modified']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Add setup file, enhance text converter with co...\n",
       "1     Add command line argument options and setup file\n",
       "2    Refactor text converter and PDF interpreter to...\n",
       "3    Bump version and add support for Chinese chara...\n",
       "4    Enhance debugging capabilities and fix errors ...\n",
       "Name: rectified_commit_msg, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Creating the master csv file\n",
    "\n",
    "file1 = pd.read_csv(\"rectified_msg.csv\")   \n",
    "file2 = pd.read_csv(\"llm_inference.csv\")   \n",
    "file3 = pd.read_csv(\"potential_bug_fix_commits.csv\")  \n",
    "\n",
    "\n",
    "agg_file2 = file2.groupby(\"hash\").agg({\n",
    "    \"filename\": lambda x: \" \".join(x.astype(str)),\n",
    "    \"llm_inference\": lambda x: \" \".join(x.astype(str)),\n",
    "    \"diff\": lambda x: \" \".join(x.astype(str)),\n",
    "    \"rectified_msg\": lambda x: \" \".join(x.dropna().astype(str))\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "master_df = (\n",
    "    file1\n",
    "    .merge(agg_file2, on=\"hash\", how=\"left\")\n",
    "    .merge(file3, on=\"hash\", how=\"left\")\n",
    ")\n",
    "\n",
    "\n",
    "master_df.to_csv(\"master_commits.csv\", index=False)\n",
    "\n",
    "print(\"Master DataFrame created and saved to master_commits.csv\")\n",
    "print(\"Columns:\", master_df.columns.tolist())\n",
    "# print(master_df.head()\n",
    "master_df[\"rectified_commit_msg\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "374ebf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hash', 'developer_msg', 'rectified_commit_msg', 'filename',\n",
      "       'llm_inference', 'diff_x', 'message', 'parents', 'is_merge', 'diff_y',\n",
      "       'files_modified'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>developer_msg</th>\n",
       "      <th>rectified_commit_msg</th>\n",
       "      <th>filename</th>\n",
       "      <th>llm_inference</th>\n",
       "      <th>diff_x</th>\n",
       "      <th>message</th>\n",
       "      <th>parents</th>\n",
       "      <th>is_merge</th>\n",
       "      <th>diff_y</th>\n",
       "      <th>files_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f719b6115b9638a8d5c6789ab29caae7e163e145</td>\n",
       "      <td>fix regex</td>\n",
       "      <td>Add setup file, enhance text converter with co...</td>\n",
       "      <td>converter.py setup.py</td>\n",
       "      <td>add more comments to text converter add missin...</td>\n",
       "      <td>@@ -364,7 +364,7 @@ class TextConverter(PDFCon...</td>\n",
       "      <td>fix regex</td>\n",
       "      <td>['1c84f1fe75f18caa55c0ff40f2fdaca1825f03c0']</td>\n",
       "      <td>False</td>\n",
       "      <td>@@ -364,7 +364,7 @@ class TextConverter(PDFCon...</td>\n",
       "      <td>['converter.py', 'setup.py']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ef06a7fd3ab366ebe6c8b11d5008211d87e3efb</td>\n",
       "      <td>fix args</td>\n",
       "      <td>Add command line argument options and setup file</td>\n",
       "      <td>pdf2zh.py setup.py</td>\n",
       "      <td>add more options to the create_parser function...</td>\n",
       "      <td>@@ -103,26 +103,26 @@ def create_parser() -&gt; a...</td>\n",
       "      <td>fix args</td>\n",
       "      <td>['f719b6115b9638a8d5c6789ab29caae7e163e145']</td>\n",
       "      <td>False</td>\n",
       "      <td>@@ -103,26 +103,26 @@ def create_parser() -&gt; a...</td>\n",
       "      <td>['pdf2zh.py', 'setup.py']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>270c0e200d1fe1666e6057ec94bedb6e0bc434fb</td>\n",
       "      <td>fix lines and indent</td>\n",
       "      <td>Refactor text converter and PDF interpreter to...</td>\n",
       "      <td>converter.py pdfinterp.py</td>\n",
       "      <td>add more examples to text converter add missin...</td>\n",
       "      <td>@@ -359,8 +359,11 @@ class TextConverter(PDFCo...</td>\n",
       "      <td>fix lines and indent</td>\n",
       "      <td>['1478a0eecbb8933410cedeff48ac9f844acd3ae4']</td>\n",
       "      <td>False</td>\n",
       "      <td>@@ -359,8 +359,11 @@ class TextConverter(PDFCo...</td>\n",
       "      <td>['converter.py', 'pdfinterp.py']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>291eebd8dcb1206e7e2e5187b13a96d138a6b1b5</td>\n",
       "      <td>fix rt</td>\n",
       "      <td>Bump version and add support for Chinese chara...</td>\n",
       "      <td>__init__.py converter.py</td>\n",
       "      <td>update version add support for 公公公公公公公公公公公公公公公...</td>\n",
       "      <td>@@ -1,2 +1,2 @@\\n-__version__ = \"1.0.1\"\\n+__ve...</td>\n",
       "      <td>fix rt</td>\n",
       "      <td>['0fa56a1c75d1dfc6fb33d46a0baa5d69ea047eb5']</td>\n",
       "      <td>False</td>\n",
       "      <td>@@ -1,2 +1,2 @@\\n-__version__ = \"1.0.1\"\\n+__ve...</td>\n",
       "      <td>['__init__.py', 'converter.py']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ac2e14192cdfee974bf8333688270c24453a8ebe</td>\n",
       "      <td>debug</td>\n",
       "      <td>Enhance debugging capabilities and fix errors ...</td>\n",
       "      <td>__init__.py cmapdb.py converter.py encodingdb....</td>\n",
       "      <td>update version add missing classes to cmap add...</td>\n",
       "      <td>@@ -1,2 +1,2 @@\\n-__version__ = \"1.0.4\"\\n+__ve...</td>\n",
       "      <td>debug</td>\n",
       "      <td>['ad7bcc919f064489de1e217cfbcb34bad5b44e5e']</td>\n",
       "      <td>False</td>\n",
       "      <td>@@ -1,2 +1,2 @@\\n-__version__ = \"1.0.4\"\\n+__ve...</td>\n",
       "      <td>['__init__.py', 'cmapdb.py', 'converter.py', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       hash         developer_msg  \\\n",
       "0  f719b6115b9638a8d5c6789ab29caae7e163e145             fix regex   \n",
       "1  1ef06a7fd3ab366ebe6c8b11d5008211d87e3efb              fix args   \n",
       "2  270c0e200d1fe1666e6057ec94bedb6e0bc434fb  fix lines and indent   \n",
       "3  291eebd8dcb1206e7e2e5187b13a96d138a6b1b5                fix rt   \n",
       "4  ac2e14192cdfee974bf8333688270c24453a8ebe                 debug   \n",
       "\n",
       "                                rectified_commit_msg  \\\n",
       "0  Add setup file, enhance text converter with co...   \n",
       "1   Add command line argument options and setup file   \n",
       "2  Refactor text converter and PDF interpreter to...   \n",
       "3  Bump version and add support for Chinese chara...   \n",
       "4  Enhance debugging capabilities and fix errors ...   \n",
       "\n",
       "                                            filename  \\\n",
       "0                              converter.py setup.py   \n",
       "1                                 pdf2zh.py setup.py   \n",
       "2                          converter.py pdfinterp.py   \n",
       "3                           __init__.py converter.py   \n",
       "4  __init__.py cmapdb.py converter.py encodingdb....   \n",
       "\n",
       "                                       llm_inference  \\\n",
       "0  add more comments to text converter add missin...   \n",
       "1  add more options to the create_parser function...   \n",
       "2  add more examples to text converter add missin...   \n",
       "3  update version add support for 公公公公公公公公公公公公公公公...   \n",
       "4  update version add missing classes to cmap add...   \n",
       "\n",
       "                                              diff_x               message  \\\n",
       "0  @@ -364,7 +364,7 @@ class TextConverter(PDFCon...             fix regex   \n",
       "1  @@ -103,26 +103,26 @@ def create_parser() -> a...              fix args   \n",
       "2  @@ -359,8 +359,11 @@ class TextConverter(PDFCo...  fix lines and indent   \n",
       "3  @@ -1,2 +1,2 @@\\n-__version__ = \"1.0.1\"\\n+__ve...                fix rt   \n",
       "4  @@ -1,2 +1,2 @@\\n-__version__ = \"1.0.4\"\\n+__ve...                 debug   \n",
       "\n",
       "                                        parents  is_merge  \\\n",
       "0  ['1c84f1fe75f18caa55c0ff40f2fdaca1825f03c0']     False   \n",
       "1  ['f719b6115b9638a8d5c6789ab29caae7e163e145']     False   \n",
       "2  ['1478a0eecbb8933410cedeff48ac9f844acd3ae4']     False   \n",
       "3  ['0fa56a1c75d1dfc6fb33d46a0baa5d69ea047eb5']     False   \n",
       "4  ['ad7bcc919f064489de1e217cfbcb34bad5b44e5e']     False   \n",
       "\n",
       "                                              diff_y  \\\n",
       "0  @@ -364,7 +364,7 @@ class TextConverter(PDFCon...   \n",
       "1  @@ -103,26 +103,26 @@ def create_parser() -> a...   \n",
       "2  @@ -359,8 +359,11 @@ class TextConverter(PDFCo...   \n",
       "3  @@ -1,2 +1,2 @@\\n-__version__ = \"1.0.1\"\\n+__ve...   \n",
       "4  @@ -1,2 +1,2 @@\\n-__version__ = \"1.0.4\"\\n+__ve...   \n",
       "\n",
       "                                      files_modified  \n",
       "0                       ['converter.py', 'setup.py']  \n",
       "1                          ['pdf2zh.py', 'setup.py']  \n",
       "2                   ['converter.py', 'pdfinterp.py']  \n",
       "3                    ['__init__.py', 'converter.py']  \n",
       "4  ['__init__.py', 'cmapdb.py', 'converter.py', '...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"master_commits.csv\")\n",
    "df = df.dropna(axis=1, how='all')\n",
    "print(df.columns)   \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474f25f",
   "metadata": {},
   "source": [
    "## Evaluation research Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ebe616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load CodeBERT\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    return output.last_hidden_state[:,0,:]\n",
    "\n",
    "\n",
    "def similarity(emb1, emb2):\n",
    "    return F.cosine_similarity(emb1, emb2).item()\n",
    "\n",
    "def normalize_text(val):\n",
    "    if isinstance(val, list):\n",
    "        return \" \".join(str(x) for x in val)\n",
    "    elif isinstance(val, str):\n",
    "        return val\n",
    "    else:\n",
    "        return \"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa1f5a",
   "metadata": {},
   "source": [
    "### Developer Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4e92a1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 335, Hits: 130, Accuracy: 38.81%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"master_commits.csv\")\n",
    "\n",
    "total = 0\n",
    "hits = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    \n",
    "    inf_text = normalize_text(row[\"diff_y\"])\n",
    "    msg_text = normalize_text(row[\"developer_msg\"])\n",
    "\n",
    "    # if not inf_text or not msg_text:\n",
    "    #     continue  \n",
    "\n",
    "    sim = similarity(get_embedding(inf_text), get_embedding(msg_text))\n",
    "\n",
    "    if sim > 0.95:\n",
    "        hits += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "print(f\"Total: {total}, Hits: {hits}, Accuracy: {hits/total*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51581ae6",
   "metadata": {},
   "source": [
    "### LLM Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cc5a2ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 335, Hits: 142, Accuracy: 42.39%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"master_commits.csv\")\n",
    "\n",
    "def normalize_text(val):\n",
    "    if isinstance(val, list):\n",
    "        return \" \".join(str(x) for x in val)\n",
    "    elif isinstance(val, str):\n",
    "        return val\n",
    "    else:\n",
    "        return \"\"  \n",
    "\n",
    "total = 0\n",
    "hits = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    \n",
    "    inf_text = normalize_text(row[\"diff_y\"])\n",
    "    msg_text = normalize_text(row[\"llm_inference\"])\n",
    "\n",
    "    # if not inf_text or not msg_text:\n",
    "    #     continue  \n",
    "\n",
    "    sim = similarity(get_embedding(inf_text), get_embedding(msg_text))\n",
    "\n",
    "    if sim > 0.95:\n",
    "        hits += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "print(f\"Total: {total}, Hits: {hits}, Accuracy: {hits/total*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0c24d",
   "metadata": {},
   "source": [
    "### Rectifier Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8a6732d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 335, Hits: 217, Accuracy: 64.78%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"master_commits.csv\")\n",
    "\n",
    "def normalize_text(val):\n",
    "    if isinstance(val, list):\n",
    "        return \" \".join(str(x) for x in val)\n",
    "    elif isinstance(val, str):\n",
    "        return val\n",
    "    else:\n",
    "        return \"\"  \n",
    "\n",
    "total = 0\n",
    "hits = 0\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    \n",
    "    inf_text = normalize_text(row[\"diff_y\"])\n",
    "    msg_text = normalize_text(row[\"rectified_commit_msg\"])\n",
    "    # print(row[\"rectified_msg\"])\n",
    "    # print(msg_text)\n",
    "\n",
    "    # if not inf_text or not msg_text:\n",
    "    #     continue  \n",
    "\n",
    "    sim = similarity(get_embedding(inf_text), get_embedding(msg_text))\n",
    "\n",
    "    if sim > 0.95:\n",
    "        hits += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "print(f\"Total: {total}, Hits: {hits}, Accuracy: {hits/total*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e171cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d38ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07df93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
